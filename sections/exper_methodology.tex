\input{sections/dataset.tex}

%\vspace{-11pt}
\subsection{Experimental Methodology}
\label{method:sec}

\subsubsection{RQ1. Comparison with Existing Approaches on C\# Dataset}

%\noindent\textbf{RQ1. Comparison with Existing Approaches on C\# Dataset.}

{\em Baselines.} We compared {\tool} against the
state-of-the-art commit-untangling approaches that work on C\#
(see more details in Section~\ref{related:sec}): Barnett {\em et
al.}~\cite{barnett-icse15}, Herzig {\em et al.}~\cite{kim-emse16},
Flexeme~\cite{flexeme-fse20}, and $\delta$-PDG+CV~\cite{flexeme-fse20}
($\delta$-PDG+CV is a model introduced by the authors of Flexeme via
combining their $\delta$-PDG with confidence voting in Herzig {\em et
al.}~\cite{kim-emse16}).

%See Section~\ref{intro:sec} for the details on the baselines.

%\begin{itemize}
%	\item Barnett et al. \cite{barnett-icse15}: An automatic technique for decomposing changesets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study
%	\item Herzig et al. \cite{kim-emse16}: An automatic approach with multi-predictors to untangle code changes.
%	\item $\sigma-$PDG + CV \cite{flexeme-fse20}: The approach that uses multi-version PDG to do the clustering to untangle the commits.
%	\item Flexeme \cite{flexeme-fse20}: An approach that builds a new defined name flow graph from commits, then applies 	agglomerative alustering using graph similarity to that newly built graph to untangle its commits.
%\end{itemize}

\textit{Procedure.}
We took all the commits in the C\# dataset and sorted them in the
chronological order based on the creation time of the commit logs. For
{\tool}, we then used 80\% of the oldest commits for training, 10\% of
the next oldest ones for tuning, and the latest 10\% of the commits
for testing. For the baselines, all of them do not need training,
thus, we ran them on the 10\% testing data.
%
We did not use the setting of leave-one-out by project (testing on
part of a project and training on other 8 projects). The reason is
that after sorting in a chronological order, we could not test on the
oldest project due to no training data. Even with the 2nd-5th oldest
ones, there is no sufficient training data of the commits that came
from the older projects and before the test commits in those projects.

We tuned {\tool} with autoML~\cite{NNI} for the following key
hyper-pa\-rameters to have the best performance: (1) Epoch size (100,
200, 300); (2) Batch size (64, 128, 256); (3) Learning rate (0.001,
0.003, 0.005, 0.010); (4) Vector length of word embeddings and its
output (150, 200, 250, 300). We empirically set the context size of 2
(RQ4).

%set the number of hops from the changed node to generate a context of
%size 2 (details in RQ4).

%(5) The maximum number $P$ of children nodes (4, 5, 6); (6) The maximum depth $Q$ of children nodes (3, 4, 5).

%In this RQ, we are focusing on the baselines that are workable on the C\# dataset. Within the C\# dataset, we firstly order all tangled commits in the dataset based on the latest modified time in the commit log from oldest to the newest. And then we split all of the tangled commits into 80\%, 10\%, and 10\% to be used for training, tuning, and testing for \tool, respectively. We try to use the existing tangled commits to let model learn the features and then use the newest tangled commits to test the performance of the model. As for the baselines, because they don't need model training, we only evaluate the model performance on the 10\% testing dataset for fair comparison. We use AutoML~\cite{NNI} on \tool to automatically tune hyper-parameters on the tuning dataset.

%\vspace{2pt}
%\noindent\textbf{RQ2. Comparison with Existing Approaches on Java Dataset.}

\subsubsection{RQ2. Comparison with Existing Approaches on Java Dataset}

{\em Baselines.} We compare {\tool} with the
state-of-the-art untangling approach that works on Java source code,
SmartCommit~\cite{smartcommit-fse21}, and the baselines used in their
paper: Base-1~\cite{smartcommit-fse21} (a rule-based approach that
puts all changes into one group), Base-2~\cite{smartcommit-fse21} (a
rule-based approach that puts the changes in each file into one
group), and Base-3~\cite{smartcommit-fse21} (a rule-based approach
that considers only def-use, use-use and same-enclosing-method
relations).



%\begin{itemize}
%	\item Base-1 \cite{smartcommit-fse21}: The rule-based approach that putting all changes into one group.
%	\item Base-2 \cite{smartcommit-fse21}: The rule-based approach that putting changes in each file into one group
%	\item Base-3 \cite{smartcommit-fse21}: The rule-based approach that considering only def-use, use-use and	same-enclosing-method relations.
%	\item SmartCommit \cite{smartcommit-fse21}: A graph-partitioning-based interactive approach to tangled changeset decomposition that leverages the efficiency of algorithms and the knowledge of developers.
%\end{itemize}

\textit{Procedure.}
We used the same procedure, tuning (with autoML~\cite{NNI}), and
parameters as in RQ1 on the Java dataset. The baselines do not need
training, thus, we ran them on the 10\% testing data.

%We set the context size of 2 (see RQ4).

%We also auto-tuned {\tool} with autoML~\cite{NNI}.

%and set 2 to the number of hops from the changed node to generate a context.

%In this RQ, we following the similar procedure in RQ1 to order all tangled commits based on the last modified time from oldest to the newest. And then we split the dataset into 80\%, 10\%, and 10\% to be used for training, tuning, and testing for \tool, respectively. As for all baselines in this RQ, they all don't need training dataset, so we directly evaluate the performance of them on the 10\% testing dataset. The same as RQ1, we use AutoML~\cite{NNI} on \tool to automatically tune hyper-parameters on the tuning dataset.

%\vspace{2pt}
%\noindent\textbf{RQ3. Within-Project Analysis.}

\subsubsection{RQ3. Within-Project Analysis}

We randomly selected one of the largest projects from each dataset:
the \code{CommandLine} project in C\# and
the \code{netty} project in Java. For each
project, we sorted all of the commits in the chronological order based
on commit time. We split the commits into 80\%, 10\%, 10\% for
training, tuning, and testing, and using the older data for training
and the newer data for tuning and testing. We separately trained,
fine-tuned, and tested a model under study on the commits in each
project.


%Because in all projects, there are
%insufficient data to train the model in this within-project setting
%(except for \code{CommandLine}), we reported the result running on
%the \code{CommandLine} project. For the other projects, we have the
%overfitting or underfitting problems due to insufficient training
%data.

%the commits in each project into 80%, 10%, and 10% for
%training, tuning, and testing. The oldest data is used to train the
%model while the newest data is used to test the model performance


%We used the C\# dataset in RQ1 as the dataset that used to do the analysis in this RQ. For each project in the dataset, we sorted the tangled commits based on the time in the commit log. Then we split the commits in each project into 80\%, 10\%, and 10\% for training, tuning, and testing. The oldest data is used to train the model while the newest data is used to test the model performance.

%We separately train, fine-tune, and test the model on each project in the dataset. But because the size of the dataset that the model uses each time is very small and limited after separating the data into each project, based on our effort on trying to run the model on each project, only on the project $Commandline$, the model could provide meaningful results with acceptable influence caused by over-fitting or under-fitting. So in the result section of this RQ, the results are reported only on the project $Commandline$.

%\vspace{2pt}
%\noindent\textbf{RQ4. Sensitivity Analysis.}

\subsubsection{RQ4. Sensitivity Analysis}

We built the variants of {\tool} by removing its important components,
one at a time. First, we removed from {\tool} the context by not using
the context vector when computing the vector for a changed
statement. Second, we removed from {\tool} the clone detection
component and used the resulting clusters from the code change
clustering learning model as the final results. Third, we studied the
impact of the size K of a context on the accuracy. We also studied the
impact of the data size. We used the C\# dataset and the same
setting/procedure as in RQ1.

%We first use \tool as the base model. We then remove the context vector from the model to evaluate the impact of context information. We also build the other model by removing the code clone from the base model to evaluate the impact of code clone technique. We used the C\# dataset and the same experiment setting as in RQ1.

%\vspace{2pt}
%\noindent\textbf{RQ5. Code Change Embedding Analysis.}
\subsubsection{RQ5. Code Change Embedding Analysis}

We use statistical $p$-test to confirm/refute the hypothesis that the
changed statements in the same concerns are projected nearer to one
another than the changed ones in different concerns in the
vector~space.


%XX changed statements $S$ each of which belongs to a different concern
%and each concern contains at least two changed statements.

%\vspace{3pt}
%\noindent {\bf Evaluation Metrics.}

\subsubsection*{Evaluation Metrics}

We use two evaluation metrics. First, $Accuracy^{c}$ is defined as the
percentage of the {\em changed statements} that are labeled with a
correct cluster/concern in all the statements in a commit:
$\mathbf{Accuracy^{c}}$= $\frac{\#
changed\:stmts\:w.\:correct\:clusters}{\#
all\:changed\:stmts\:in\:commit}$.  Note that a model might label the
statements with a different permutation of cluster labels than the one
in the ground truth. For example, we have five statements and three
concerns. A model can predict [3,1,1,2,2] and the ground truth has
[1,2,2,3,3]. A naive evaluation would give an accuracy of
0.0. However, a permutation of the labels for clusters would give
indeed an accuracy of 1.0. Thus, we use the Hungarian
Algorithm~\cite{hungarian-algo} to find the permutation that gives the
maximum accuracy, and use that for $Accuracy^{c}$.

In RQ1, for the comparison with Flexeme~\cite{flexeme-fse20}, we also
used another metric that was used in their paper, which is similar to
$Accuracy^{c}$ except that it considers all statements: $\mathbf{Accuracy^{a}}$
= $\frac{\# stmts\:w.\:correct\:clusters}{\# all\:stmts\:in\:commit}$.
We also consider all label permutations.

%Both baselines, as well as Heddle, may recover an arbitrary
%permutation of the ground truth labels. To avoid artificially
%penalising them, we first use the Hungarian Algorithm [15] to find the
%permutation that maximises accuracy. Consider the ground truth
%‘[01122]’, should a tool output ‘[20011]’, a naïve approach would
%award it 0.0 accuracy, while a trivial permutation of the labelling
%function reveals that this is indeed 1.0 accuracy. We report this
%maximal accuracy for each method.


%In all experiments, we measure the performance of all approaches with
%two evaluation metrics. The main metric we are using is the untangling
%accuracy $Accuracy^{(1)}$. It is defined as follow:

%\begin{equation}\label{eq7}
%	Accuracy^{(1)} = \frac{\#\:of\:Correctly\:Clustered\:Changed\:Statement}{\#\:of\:Changed\:Statement\:in\:the\:Commit}
%\end{equation}

%In RQ1, because we directly use the dataset from Partachi et al. \cite{flexeme-fse20} and compare with this study as a baseline in this research question, we also use the metric that has been used in Partachi et al.'s \cite{flexeme-fse20} study for doing the evaluation to fully analysis the differences on the performance. The accuracy metric in Partachi et al.'s \cite{flexeme-fse20} study is defined as follow:

%\begin{equation}\label{eq8}
%	Accuracy^{(2)} = \frac{\#\:of\:Correctly\: Clustered\: Changed\: Statement}{\#\: of\: Nodes\: in\: the\: Graph}
%\end{equation}
