\subsubsection{{\bf RQ4. Sensitivity Analysis}}


\begin{table}[t]
	\caption{RQ4. Impacts of Key Features on Accuracy}
	\vspace{-12pt}
	\begin{center}
		\footnotesize
		\tabcolsep 4pt
		\renewcommand{\arraystretch}{1} \begin{tabular}{p{3cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}}
			
			\hline
			       \multirow{2}{*}{}                  & \multicolumn{3}{c}{$Accuracy^c$}\\
			                         \cline{2-4}
			    \# concerns                     & 2 & 3& Overall\\
			\hline

			\tool w/o Context        &  0.38 & 0.43  &   0.40        \\
			\tool w/o Clone Detection     &  0.42 & 0.44  &   0.43        \\
       			\tool                    &  0.44 & 0.47  &   0.45        \\
			\hline
		\end{tabular}
		\label{RQ4-result-1}
	\end{center}
\end{table}

Table~\ref{RQ4-result-1} shows the accuracy results when the key
components in {\tool} were removed. As seen, while both context and
clone detection positively contribute to {\tool}'s accuracy, the
context plays a more important role as expected. Without the context
vector, the accuracy decreases by 13.7\%, 8.5\%, and 11.1\% for the
commits with two, three, and all concerns, respectively. Without the
clone detection, the accuracy decreases by 4.5\%, 6.4\%, and 4.4\% for
the commits with two, three, and all concerns, respectively.

%Table~\ref{RQ4-result-1} shows the changes to the metrics as we remove one key feature from \tool. Generally, each key feature contributes positively to the better performance of {\tool}, as the decreasing of $Accuracy^c$ when removing each of them from \tool. When {\tool} removes context information, the $Accuracy^c$ decreased by $13.7\%, 8.5\%$, and $11.1\%$ on $2$ concerns data, $3$ concerns data, and overall performance, respectively. When {\tool} removes code clone, the $Accuracy^c$ decreased by $4.5\%, 6.4\%$, and $4.4\%$ on $2$ concerns data, $3$ concerns data, and overall performance, respectively.


\begin{table}[t]
	\caption{RQ4. Impact of the Number $k$ of Hops for Context}
	\vspace{-12pt}
	\begin{center}
		\footnotesize
		\tabcolsep 4pt
		\renewcommand{\arraystretch}{1} \begin{tabular}{p{3cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}}
			
			\hline
			       \multirow{2}{*}{}                  & \multicolumn{3}{c}{$Accuracy^c$}\\
\cline{2-4}
\#concerns & 2 & 3& Overall\\
			\hline
			\tool ($k=1$)          & 0.42 & 0.45 &  0.43          \\
			\tool ($k=2$)          & 0.44 & 0.47 &  0.45          \\
			\tool ($k=3$)          & 0.43 & 0.45 &  0.44          \\
			\tool ($k=4$)          & 0.40 & 0.44 &  0.42          \\
			\tool ($k=5$)          & 0.41 & 0.44 &  0.42          \\
			\tool (Full Graph)     & 0.39 & 0.43 &  0.41          \\
			\hline
		\end{tabular}
		\label{RQ4-result-2}
	\end{center}
\end{table}

Table~\ref{RQ4-result-2} shows the accuracy when we vary the size $K$
of a context (i.e., the number of hops from the changed node). As
seen, when the number of surrounding nodes of the changed node
increases from $K$=1--2, the accuracy increases and reaches its peak
at $K$=2. As $K$=1, the immediate surrounding nodes cannot capture
well the relevant nodes for {\tool} to distinguish the concerns of the
changed statements. With two hops from the changed node, the context
seems to sufficiently contain the crucial nodes w.r.t. determining
the concerns. However, as the size continues to increase, the accuracy
decreases. When the entire graph is used as the context, the accuracy
is the lowest. If more nodes are considered in the context, more noises 
are added as more irrelevant data is used.  The trend is the same for
the commits with two, three, or all concerns.

%Table~\ref{RQ4-result-2} shows that selecting $k$-neighbors for
%context in \tool, $k=2$ is the best choice for \tool because of the
%highest $Accuracy^c$ on $2$ concerns data, $3$ concerns data and
%overall performance. Even though in this table, we only show the $k$
%value range from $1$ to $5$.

%However, from the trend of $Accuracy^c$ changing, we can see that if
%$k>=2$, when $k$ increases, the $Accuracy^c$ decreases with the reason
%that when $k$ increases, the newly added nodes have less relationship
%with the entire node and may bring more biases. Based on this, even
%compared with the untested $k$ value, $k=2$ is still the best setting
%for \tool.

\begin{table}[t]
%	\caption{Top-1 with Data Splitting on CPatMiner dataset}
\caption{Impact of the Size of Training Data}
	\vspace{-12pt}
	\tabcolsep 2pt
	\small
	\begin{center}
\begin{tabular}{|c|l|l|l|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  Splitting on C\# dataset & 80\%/10\%/10\% & 70\%/15\%/15\% & 60\%/20\%/20\% \\
  \hline
  \% $Accuracy^{c}$ & 45\% & 42\% & 37\% \\
  \hline
\end{tabular}
\label{splitting}
	\end{center}
\vspace{-3pt}
\end{table}

As seen in Table~\ref{splitting}, the size of training data has impact
on accuracy. The more training data, the higher the accuracy.  Even
reducing from 80\% to 60\%, {\tool}'s accuracy is still higher than
Flexeme's.
